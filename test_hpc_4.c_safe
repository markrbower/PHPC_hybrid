#include <iostream>
#include <stdio.h>
#include <vector>
#include <cstdlib>
#include <string>
#include <mpi.h>
#include <omp.h>
#include <openacc.h>
#include <time.h>
#include "timer.h"

#define NSIZE 80000000

int main(int argc, char *argv[]) {
  int ntimes=16;
  int sumAll=0;
  int n_elements_received;
  struct timespec tstart;
  double scalar = 3.0, time_sum = 0.0;
  int np, rank, namelen;
  char processor_name[MPI_MAX_PROCESSOR_NAME];
  int tmp;
  double dtmp;


  double* restrict a = (double*)malloc(NSIZE * sizeof(double));
  double* restrict b = (double*)malloc(NSIZE * sizeof(double));
  double* restrict c = (double*)malloc(NSIZE * sizeof(double));
  double* restrict a2 = (double*)malloc(NSIZE * sizeof(double));
  double* restrict b2 = (double*)malloc(NSIZE * sizeof(double));

  void* va = (void*)malloc(NSIZE*sizeof(double));
  void* vb = (void*)malloc(NSIZE*sizeof(double));

  omp_set_num_threads(4);

  #pragma acc parallel loop present( a[0:NSIZE], b[0:NSIZE], c[0:NSIZE] )
  for (int i=0; i<NSIZE; i++) {
	a[i] = 1.0;
       	b[i] = 2.0;
  }

  MPI_Status status;

  MPI_Init(&argc, &argv);
  MPI_Comm_size(MPI_COMM_WORLD, &np);
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Get_processor_name(processor_name, &namelen);

  MPI_Barrier(MPI_COMM_WORLD);

  for (int k=0; k<ntimes; k++ ) {
       if ( rank == 0 ) { // Manager
		int index,i;
  	    	int elements_per_process = NSIZE / np;

		if ( np > 1 ) {
			for (i = 1; i < np - 1; i++) {
	                	index = i * elements_per_process;
  
       	        		MPI_Send(&elements_per_process,
                        		1, MPI_INT, i, 0,
                        		MPI_COMM_WORLD);
                		MPI_Send(&a[index],
                        		elements_per_process,
                        		MPI_INT, i, 0,
                        		MPI_COMM_WORLD);
                		MPI_Send(&b[index],
                        		elements_per_process,
                        		MPI_INT, i, 0,
                        		MPI_COMM_WORLD);
            		}
			// send remaining elements to last processor
            		index = i * elements_per_process;
            		int elements_left = NSIZE - index;
  
            		MPI_Send(&elements_left,
                     		1, MPI_INT,
                     		i, 0,
                     		MPI_COMM_WORLD);
            		MPI_Send(&a[index],
                     		elements_left,
                     		MPI_DOUBLE, i, 0,
                     		MPI_COMM_WORLD);
            		MPI_Send(&b[index],
                     		elements_left,
                     		MPI_DOUBLE, i, 0,
                     		MPI_COMM_WORLD);
		} // np > 0	
		// master process add its own sub array
//		printf("M\n");
		sumAll = 0.0;
		#pragma acc parallel loop gang device_type(acc_device_nvidia) vector_length(256)
        	for (int i = 0; i < elements_per_process; i++)
            		sumAll += a[i] + scalar*b[i];

           	time_sum = cpu_timer_stop(tstart);
  
        	// collects partial sums from other processes
        	for (i = 1; i < np; i++) {
			printf( "Manager receiving %d.\n", i );
            		MPI_Recv(&tmp, 1, MPI_INT,
                     		MPI_ANY_SOURCE, 0,
                     		MPI_COMM_WORLD,
                     		&status);
            		MPI_Recv(&dtmp, 1, MPI_DOUBLE,
                     		MPI_ANY_SOURCE, 0,
                     		MPI_COMM_WORLD,
                     		&status);
            		sumAll += tmp;
			time_sum += dtmp;
        	}
        	// prints the final sum of array
        	printf("Sum of array is : %d\n", sumAll);
        	printf("Time sum is : %d\n", time_sum );
       } else { // Worker
//	        printf("W\n");
        	MPI_Recv(&n_elements_received,
                 	1, MPI_INT, 0, 0,
                 	MPI_COMM_WORLD,
                 	&status);
  
        	// stores the received array segment
        	// in local array a2 & b2
//	        printf("a: %d\n", n_elements_received);
        	MPI_Recv( va, n_elements_received,
                 	MPI_DOUBLE, 0, 0,
                 	MPI_COMM_WORLD,
                 	&status);
//	        printf("Wb\n");
        	MPI_Recv( vb, n_elements_received,
                 	MPI_DOUBLE, 0, 0,
                 	MPI_COMM_WORLD,
                 	&status);
//	        printf("R\n");

        	// calculates its partial sum
        	double partial_sum = 0.0;
       		cpu_timer_start(&tstart);
		a2 = (double*) va;
		b2 = (double*) vb;
		#pragma acc parallel loop gang device_type(acc_device_nvidia) vector_length(256)
        	for (int i = 0; i < n_elements_received; i++)
            		partial_sum += a2[i] + scalar*b2[i];

           	time_sum = cpu_timer_stop(tstart);
  
        	// sends the partial sum to the root process
		printf( "Worker %d sending.\n", rank );
        	MPI_Send(&partial_sum, 1, MPI_INT,
                 	0, 0, MPI_COMM_WORLD);
        	MPI_Send(&time_sum, 1, MPI_DOUBLE,
                 	0, 0, MPI_COMM_WORLD);
       }
  }

  printf("Average runtime for stream triad loop is %lf msecs\n", time_sum/ntimes);

  free(a);
  free(b);
  free(c);
  free(a2);
  free(b2);

  MPI_Finalize();
  return(0);
}


